{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2e6e025-bdd0-47a0-9243-6e85b4e1ff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81e9212-f721-4fc3-a71b-21cc093299ea",
   "metadata": {},
   "source": [
    "#Классификация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38ea8d6b-c34b-4f6b-b0b6-ef343b1a5af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "military uniform: 33.8%\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "img = read_image(\"assets/2.jpg\")\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "model = resnet50(weights=weights)\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "batch = preprocess(img).unsqueeze(0)\n",
    "\n",
    "# Step 4: Use the model and print the predicted category\n",
    "prediction = model(batch).squeeze(0).softmax(0)\n",
    "class_id = prediction.argmax().item()\n",
    "score = prediction[class_id].item()\n",
    "category_name = weights.meta[\"categories\"][class_id]\n",
    "print(f\"{category_name}: {100 * score:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5f1cf6-f81a-4dba-a113-2ac1bfc9e634",
   "metadata": {},
   "source": [
    "#Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "482838a6-12a0-4cdc-9b34-35fa361aad61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "military uniform: 51.280707120895386%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matthew\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_utils.py:404: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models.quantization import resnet50, ResNet50_QuantizedWeights\n",
    "\n",
    "img = read_image(\"assets/2.jpg\")\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = ResNet50_QuantizedWeights.DEFAULT\n",
    "model = resnet50(weights=weights, quantize=True)\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "batch = preprocess(img).unsqueeze(0)\n",
    "\n",
    "# Step 4: Use the model and print the predicted category\n",
    "prediction = model(batch).squeeze(0).softmax(0)\n",
    "class_id = prediction.argmax().item()\n",
    "score = prediction[class_id].item()\n",
    "category_name = weights.meta[\"categories\"][class_id]\n",
    "print(f\"{category_name}: {100 * score}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d87d8e5-8213-41e4-b305-7e893e2eda00",
   "metadata": {},
   "source": [
    "#Semantic segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e326cb76-ec5b-44f8-81cb-c28978ef4034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "img = read_image(\"assets/3.jpg\")\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = FCN_ResNet50_Weights.DEFAULT\n",
    "model = fcn_resnet50(weights=weights)\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "batch = preprocess(img).unsqueeze(0)\n",
    "\n",
    "print(weights.meta[\"categories\"]);\n",
    "\n",
    "# Step 4: Use the model and visualize the prediction\n",
    "prediction = model(batch)[\"out\"]\n",
    "normalized_masks = prediction.softmax(dim=1)\n",
    "class_to_idx = {cls: idx for (idx, cls) in enumerate(weights.meta[\"categories\"])}\n",
    "mask = normalized_masks[0, class_to_idx[\"person\"]]\n",
    "to_pil_image(mask).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da828cd-45e3-4311-9f9f-3c89821bcf2a",
   "metadata": {},
   "source": [
    "#Object Detection, Instance Segmentation and Person Keypoint Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ba8aa9f-8d29-4c0b-9364-afac2d2bdd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matthew\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\utils.py:225: UserWarning: Argument 'font_size' will be ignored since 'font' is not set.\n",
      "  warnings.warn(\"Argument 'font_size' will be ignored since 'font' is not set.\")\n"
     ]
    }
   ],
   "source": [
    "from torchvision.io.image import read_image\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "img = read_image(\"assets/3.jpg\")\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "batch = [preprocess(img)]\n",
    "\n",
    "# Step 4: Use the model and visualize the prediction\n",
    "prediction = model(batch)[0]\n",
    "labels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "box = draw_bounding_boxes(img, boxes=prediction[\"boxes\"],\n",
    "                          labels=labels,\n",
    "                          colors=\"red\",\n",
    "                          width=4, font_size=30)\n",
    "im = to_pil_image(box.detach())\n",
    "im.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a744ed-76a5-4f50-bf9c-d779475b0283",
   "metadata": {},
   "source": [
    "#Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da90f04f-10e0-452f-ae1f-b91530bf1b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matthew\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\io\\video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disc golfing: 61.619532108306885%\n"
     ]
    }
   ],
   "source": [
    "from torchvision.io.video import read_video\n",
    "from torchvision.models.video import mvit_v2_s, MViT_V2_S_Weights\n",
    "\n",
    "vid, _, _ = read_video(\"assets/vid2.mp4\", output_format=\"TCHW\")\n",
    "vid = vid[:16]  # optionally shorten duration\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = MViT_V2_S_Weights.KINETICS400_V1\n",
    "model = mvit_v2_s(weights=weights)\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "batch = preprocess(vid).unsqueeze(0)\n",
    "\n",
    "# Step 4: Use the model and print the predicted category\n",
    "prediction = model(batch).squeeze(0).softmax(0)\n",
    "label = prediction.argmax().item()\n",
    "score = prediction[label].item()\n",
    "category_name = weights.meta[\"categories\"][label]\n",
    "print(f\"{category_name}: {100 * score}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ebd2dd-6f2a-4eb7-be69-af614d0111aa",
   "metadata": {},
   "source": [
    "#Трассировка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bf134d6-8228-40a4-bc16-c5874dc7ca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d1e432a-c13f-448c-909a-f4aee7ca5ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(x, y):\n",
    "    return 2 * x + y\n",
    "\n",
    "# Run `foo` with the provided inputs and record the tensor operations\n",
    "traced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))\n",
    "\n",
    "# `traced_foo` can now be run with the TorchScript interpreter or saved\n",
    "# and loaded in a Python-free environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43930e71-2e82-4db4-88f9-6d06a396f65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(1, 1, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "n = Net()\n",
    "example_weight = torch.rand(1, 1, 3, 3)\n",
    "example_forward_input = torch.rand(1, 1, 3, 3)\n",
    "\n",
    "# Trace a specific method and construct `ScriptModule` with\n",
    "# a single `forward` method\n",
    "module = torch.jit.trace(n.forward, example_forward_input)\n",
    "\n",
    "# Trace a module (implicitly traces `forward`) and construct a\n",
    "# `ScriptModule` with a single `forward` method\n",
    "module = torch.jit.trace(n, example_forward_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1c64920-25a3-4bc0-bbd2-b4fdd3496ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3830])\n"
     ]
    }
   ],
   "source": [
    "x1, h1 = torch.rand(1), torch.rand(1)\n",
    "print(traced_foo(x1,h1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d7e514-03b8-421a-8be3-420dce79b267",
   "metadata": {},
   "source": [
    "#TorchScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bac8fcc-e765-4154-845f-14bbdeba1ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x13474634630>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch  # This is all you need to use both PyTorch and TorchScript!\n",
    "print(torch.__version__)\n",
    "torch.manual_seed(191009)  # set the seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bd397b7-76d2-4820-afa8-8b4159984f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0.8219, 0.8990, 0.6670, 0.8277],\n",
      "        [0.5176, 0.4017, 0.8545, 0.7336],\n",
      "        [0.6013, 0.6992, 0.2618, 0.6668]]), tensor([[0.8219, 0.8990, 0.6670, 0.8277],\n",
      "        [0.5176, 0.4017, 0.8545, 0.7336],\n",
      "        [0.6013, 0.6992, 0.2618, 0.6668]]))\n"
     ]
    }
   ],
   "source": [
    "class MyCell(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCell, self).__init__()\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        new_h = torch.tanh(x + h)\n",
    "        return new_h, new_h\n",
    "\n",
    "my_cell = MyCell()\n",
    "x = torch.rand(3, 4)\n",
    "h = torch.rand(3, 4)\n",
    "print(my_cell(x, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "812dc2ec-2496-437f-861e-d2aa016991e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCell(\n",
      "  (linear): Linear(in_features=4, out_features=4, bias=True)\n",
      ")\n",
      "(tensor([[ 0.8573,  0.6190,  0.5774,  0.7869],\n",
      "        [ 0.3326,  0.0530,  0.0702,  0.8114],\n",
      "        [ 0.7818, -0.0506,  0.4039,  0.7967]], grad_fn=<TanhBackward0>), tensor([[ 0.8573,  0.6190,  0.5774,  0.7869],\n",
      "        [ 0.3326,  0.0530,  0.0702,  0.8114],\n",
      "        [ 0.7818, -0.0506,  0.4039,  0.7967]], grad_fn=<TanhBackward0>))\n"
     ]
    }
   ],
   "source": [
    "class MyCell(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCell, self).__init__()\n",
    "        self.linear = torch.nn.Linear(4, 4)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        new_h = torch.tanh(self.linear(x) + h)\n",
    "        return new_h, new_h\n",
    "\n",
    "my_cell = MyCell()\n",
    "print(my_cell)\n",
    "print(my_cell(x, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c3690ae-7634-4d9d-bc66-bf4a1bd6d6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCell(\n",
      "  (dg): MyDecisionGate()\n",
      "  (linear): Linear(in_features=4, out_features=4, bias=True)\n",
      ")\n",
      "(tensor([[ 0.8346,  0.5931,  0.2097,  0.8232],\n",
      "        [ 0.2340, -0.1254,  0.2679,  0.8064],\n",
      "        [ 0.6231,  0.1494, -0.3110,  0.7865]], grad_fn=<TanhBackward0>), tensor([[ 0.8346,  0.5931,  0.2097,  0.8232],\n",
      "        [ 0.2340, -0.1254,  0.2679,  0.8064],\n",
      "        [ 0.6231,  0.1494, -0.3110,  0.7865]], grad_fn=<TanhBackward0>))\n"
     ]
    }
   ],
   "source": [
    "class MyDecisionGate(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        if x.sum() > 0:\n",
    "            return x\n",
    "        else:\n",
    "            return -x\n",
    "\n",
    "class MyCell(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCell, self).__init__()\n",
    "        self.dg = MyDecisionGate()\n",
    "        self.linear = torch.nn.Linear(4, 4)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        new_h = torch.tanh(self.dg(self.linear(x)) + h)\n",
    "        return new_h, new_h\n",
    "\n",
    "my_cell = MyCell()\n",
    "print(my_cell)\n",
    "print(my_cell(x, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "834582e3-ee01-41b0-bc79-df4319aad7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCell(\n",
      "  original_name=MyCell\n",
      "  (linear): Linear(original_name=Linear)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.2541,  0.2460,  0.2297,  0.1014],\n",
       "         [-0.2329, -0.2911,  0.5641,  0.5015],\n",
       "         [ 0.1688,  0.2252,  0.7251,  0.2530]], grad_fn=<TanhBackward0>),\n",
       " tensor([[-0.2541,  0.2460,  0.2297,  0.1014],\n",
       "         [-0.2329, -0.2911,  0.5641,  0.5015],\n",
       "         [ 0.1688,  0.2252,  0.7251,  0.2530]], grad_fn=<TanhBackward0>))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyCell(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCell, self).__init__()\n",
    "        self.linear = torch.nn.Linear(4, 4)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        new_h = torch.tanh(self.linear(x) + h)\n",
    "        return new_h, new_h\n",
    "\n",
    "my_cell = MyCell()\n",
    "x, h = torch.rand(3, 4), torch.rand(3, 4)\n",
    "traced_cell = torch.jit.trace(my_cell, (x, h))\n",
    "print(traced_cell)\n",
    "traced_cell(x, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db185415-27d2-45a8-a776-eb5f2332c96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-0.2541,  0.2460,  0.2297,  0.1014],\n",
      "        [-0.2329, -0.2911,  0.5641,  0.5015],\n",
      "        [ 0.1688,  0.2252,  0.7251,  0.2530]], grad_fn=<TanhBackward0>), tensor([[-0.2541,  0.2460,  0.2297,  0.1014],\n",
      "        [-0.2329, -0.2911,  0.5641,  0.5015],\n",
      "        [ 0.1688,  0.2252,  0.7251,  0.2530]], grad_fn=<TanhBackward0>))\n",
      "(tensor([[-0.2541,  0.2460,  0.2297,  0.1014],\n",
      "        [-0.2329, -0.2911,  0.5641,  0.5015],\n",
      "        [ 0.1688,  0.2252,  0.7251,  0.2530]], grad_fn=<TanhBackward0>), tensor([[-0.2541,  0.2460,  0.2297,  0.1014],\n",
      "        [-0.2329, -0.2911,  0.5641,  0.5015],\n",
      "        [ 0.1688,  0.2252,  0.7251,  0.2530]], grad_fn=<TanhBackward0>))\n"
     ]
    }
   ],
   "source": [
    "print(my_cell(x, h))\n",
    "print(traced_cell(x, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d3855db-b506-4ea6-83ac-7adb01672db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    argument_1: Tensor) -> Tensor:\n",
      "  return torch.neg(argument_1)\n",
      "\n",
      "def forward(self,\n",
      "    x: Tensor,\n",
      "    h: Tensor) -> Tuple[Tensor, Tensor]:\n",
      "  dg = self.dg\n",
      "  linear = self.linear\n",
      "  _0 = torch.add((dg).forward((linear).forward(x, ), ), h)\n",
      "  _1 = torch.tanh(_0)\n",
      "  return (_1, _1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matthew\\AppData\\Local\\Temp\\ipykernel_11904\\4234398751.py:3: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if x.sum() > 0:\n"
     ]
    }
   ],
   "source": [
    "class MyDecisionGate(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        if x.sum() > 0:\n",
    "            return x\n",
    "        else:\n",
    "            return -x\n",
    "\n",
    "class MyCell(torch.nn.Module):\n",
    "    def __init__(self, dg):\n",
    "        super(MyCell, self).__init__()\n",
    "        self.dg = dg\n",
    "        self.linear = torch.nn.Linear(4, 4)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        new_h = torch.tanh(self.dg(self.linear(x)) + h)\n",
    "        return new_h, new_h\n",
    "\n",
    "my_cell = MyCell(MyDecisionGate())\n",
    "traced_cell = torch.jit.trace(my_cell, (x, h))\n",
    "\n",
    "print(traced_cell.dg.code)\n",
    "print(traced_cell.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "312524ff-1d73-4f8e-a34e-8c10db5d6f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    x: Tensor) -> Tensor:\n",
      "  if bool(torch.gt(torch.sum(x), 0)):\n",
      "    _0 = x\n",
      "  else:\n",
      "    _0 = torch.neg(x)\n",
      "  return _0\n",
      "\n",
      "def forward(self,\n",
      "    x: Tensor,\n",
      "    h: Tensor) -> Tuple[Tensor, Tensor]:\n",
      "  dg = self.dg\n",
      "  linear = self.linear\n",
      "  _0 = torch.add((dg).forward((linear).forward(x, ), ), h)\n",
      "  new_h = torch.tanh(_0)\n",
      "  return (new_h, new_h)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scripted_gate = torch.jit.script(MyDecisionGate())\n",
    "\n",
    "my_cell = MyCell(scripted_gate)\n",
    "scripted_cell = torch.jit.script(my_cell)\n",
    "\n",
    "print(scripted_gate.code)\n",
    "print(scripted_cell.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59383a29-3ce0-490c-84a2-1ffa46a61d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 0.1404,  0.8031,  0.1122,  0.7944],\n",
      "        [ 0.4230,  0.4897,  0.3859,  0.6778],\n",
      "        [ 0.8192,  0.7816, -0.4313,  0.8242]], grad_fn=<TanhBackward0>), tensor([[ 0.1404,  0.8031,  0.1122,  0.7944],\n",
      "        [ 0.4230,  0.4897,  0.3859,  0.6778],\n",
      "        [ 0.8192,  0.7816, -0.4313,  0.8242]], grad_fn=<TanhBackward0>))\n"
     ]
    }
   ],
   "source": [
    "# New inputs\n",
    "x, h = torch.rand(3, 4), torch.rand(3, 4)\n",
    "print(scripted_cell(x, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "42798c12-3bfe-42ad-b462-569bef6b9120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    xs: Tensor) -> Tuple[Tensor, Tensor]:\n",
      "  h = torch.zeros([3, 4])\n",
      "  y = torch.zeros([3, 4])\n",
      "  y0 = y\n",
      "  h0 = h\n",
      "  for i in range(torch.size(xs, 0)):\n",
      "    cell = self.cell\n",
      "    _0 = (cell).forward(torch.select(xs, 0, i), h0, )\n",
      "    y1, h1, = _0\n",
      "    y0, h0 = y1, h1\n",
      "  return (y0, h0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class MyRNNLoop(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyRNNLoop, self).__init__()\n",
    "        self.cell = torch.jit.trace(MyCell(scripted_gate), (x, h))\n",
    "\n",
    "    def forward(self, xs):\n",
    "        h, y = torch.zeros(3, 4), torch.zeros(3, 4)\n",
    "        for i in range(xs.size(0)):\n",
    "            y, h = self.cell(xs[i], h)\n",
    "        return y, h\n",
    "\n",
    "rnn_loop = torch.jit.script(MyRNNLoop())\n",
    "print(rnn_loop.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf6118c7-06ad-4281-b38d-fbc31c878c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    xs: Tensor) -> Tensor:\n",
      "  loop = self.loop\n",
      "  _0, y, = (loop).forward(xs, )\n",
      "  return torch.relu(y)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class WrapRNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WrapRNN, self).__init__()\n",
    "        self.loop = torch.jit.script(MyRNNLoop())\n",
    "\n",
    "    def forward(self, xs):\n",
    "        y, h = self.loop(xs)\n",
    "        return torch.relu(y)\n",
    "\n",
    "traced = torch.jit.trace(WrapRNN(), (torch.rand(10, 3, 4)))\n",
    "print(traced.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c992e798-e21b-4d20-8d8e-8de19b81b013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecursiveScriptModule(\n",
      "  original_name=WrapRNN\n",
      "  (loop): RecursiveScriptModule(\n",
      "    original_name=MyRNNLoop\n",
      "    (cell): RecursiveScriptModule(\n",
      "      original_name=MyCell\n",
      "      (dg): RecursiveScriptModule(original_name=MyDecisionGate)\n",
      "      (linear): RecursiveScriptModule(original_name=Linear)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "def forward(self,\n",
      "    xs: Tensor) -> Tensor:\n",
      "  loop = self.loop\n",
      "  _0, y, = (loop).forward(xs, )\n",
      "  return torch.relu(y)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "traced.save('wrapped_rnn.pt')\n",
    "\n",
    "loaded = torch.jit.load('wrapped_rnn.pt')\n",
    "\n",
    "print(loaded)\n",
    "print(loaded.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfca150-ce31-42b3-91d4-c7c2e8f8f101",
   "metadata": {},
   "source": [
    "#Выполнение задания согласно лабораторной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7fa975b6-58ab-4695-bea3-0eb7e44b9bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n"
     ]
    }
   ],
   "source": [
    "#Исходная модель по объектам\n",
    "\n",
    "from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "img = read_image(\"assets/3.jpg\")\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = FCN_ResNet50_Weights.DEFAULT\n",
    "model = fcn_resnet50(weights=weights)\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "batch = preprocess(img).unsqueeze(0)\n",
    "\n",
    "print(weights.meta[\"categories\"]);\n",
    "\n",
    "# Step 4: Use the model and visualize the prediction\n",
    "prediction = model(batch)[\"out\"]\n",
    "normalized_masks = prediction.softmax(dim=1)\n",
    "class_to_idx = {cls: idx for (idx, cls) in enumerate(weights.meta[\"categories\"])}\n",
    "mask = normalized_masks[0, class_to_idx[\"person\"]]\n",
    "to_pil_image(mask).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a070ce5d-6e60-4f61-a63c-afaa60f1c32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\n",
    "\n",
    "weights = FCN_ResNet50_Weights.DEFAULT\n",
    "model = fcn_resnet50(weights=weights)\n",
    "model.eval()  \n",
    "\n",
    "example_input = torch.rand(1, 3, 224, 224)  # 1 изображение, 3 канала (RGB), 224x224 пикселя\n",
    "traced_model = torch.jit.trace(model, example_input, strict=False)\n",
    "\n",
    "# Сохранение трассированной модели\n",
    "traced_model.save(\"traced_fcn_resnet50.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3c25d546-6aeb-4f3b-86a2-74459bc72e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка трассированной модели\n",
    "loaded_model = torch.jit.load(\"traced_fcn_resnet50.pt\")\n",
    "loaded_model.eval()\n",
    "\n",
    "batch = preprocess(img).unsqueeze(0)\n",
    "output = loaded_model(batch)\n",
    "\n",
    "prediction = output[\"out\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "90bffe9e-7a59-478c-b05f-a84e9b63ffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_masks = prediction.softmax(dim=1)\n",
    "class_to_idx = {cls: idx for (idx, cls) in enumerate(weights.meta[\"categories\"])}\n",
    "mask = normalized_masks[0, class_to_idx[\"person\"]]\n",
    "to_pil_image(mask).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
